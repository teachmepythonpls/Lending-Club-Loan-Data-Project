# Lending-Club-Loan-Data-Project
1) Import data to analytics environment and clean it. Define data quality problems of assignment data.
2) Perform initial data analysis, define its outcome. Do the necessary data wrangling to prepare and transform the data.
3) Do exploratory data analysis.
4) Apply appropriate sampling and resampling technique to reduce the volume of data to process and the same time retains properties of the data.
5) Do visual data analysis and create all required data insights.
6) Define problem to solve for the case, specify dependent and independent features, extract and engineer all the features possible for a given problem.
7) Visualize and explore the features – create data story to explain findings and data insights.
8) Design and implement machine learning models. Do experiments and select the best configuration of the model to solve data or analytic problem. Document key experiments and outcomes of models, provide results of the comparison.
9) Apply the appropriate ML model evaluation method. Explain the results of the evaluation.
    
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

# TABLEAU: (for visuals refer to - tableau)
1. Data cleaning, transformation and preparation (Tableau prep)
First of all, the csv files were “adjusted” using Excel. Since the columns in the data files were separated by quotes, it was challenging to properly load such files into Tableau prep, so I decided to remove the “,” characters and change them to ,. Then, after loading both the loan and rejected loan data files, I started to organize them sequentially. I followed this principle: First line contains headers, field separator – comma.
The second step was to remove irrelevant/unnecessary/valueless/NULL information (mostly 0 values, too long words/texts, the only value in the table, etc.). I removed all null values ​​and those columns that would have no effect or would cause unnecessary load in the final visualization stage. Columns such as, for example, url, title, zip_code, out_prncp, next_pymnt_d.
I also did some formatting of the columns, since some of them were "mixed up", I rearranged the address column, the calculation - to leave only uppercase values, i.e. state abbreviations, formatting the date column, and formatting some columns from number to text and vice versa.
After organizing all the tables and performing a "clean" step, I started merging the columns, which I divided into two parts - data on issued loans and data on rejected loans. I decided to stop at the "Union" merging option, since I made all the columns identical in both tables and combined the data for different years into one array, so this option seemed more acceptable to me. Unlike "Join", this option did not create additional columns, but connected the data from the second table to the first, and thus got one neat dataset table (both loan issued and rejected). I also left the "Join" option on to visually compare the difference. Finally, I exported the results to my desktop and imported them into Tableau Desktop.

2. Data visualization, analysis, observations and conclusions (Tableau Desktop)
1. LoanChart1 – this chart looks at the relationship between the loan amount issued and the loan status according to the grade (“grade” – indicates how risky it is for the bank to lend to this customer. A is the least risky, G is the most). I also included a verification status filter to further supplement the chart. It can be seen that the largest “Loan_Amnt” amounts (how much was lent to the customer) belong to the “Verified” status and they revolve around the A, B and C categories, while in the riskiest category G it is seen that the loan amounts are dramatically lower. Another interesting thing is the “Recoveries” (which already helps us understand that these customers are unlikely to repay the loan), which are found only in the “Charged off” columns.
2. LoanChart2 – The second chart shows the trend between short-term and long-term customers. It can be seen that the largest loan amount (all together) is received by the group of clients with 10+ years of work experience, which is another indicator that distinguishes risky and non-risky clients. Clients who have been working the longest are also attributed to the highest number of bankruptcy cases. It is also interesting to note that more loans are taken for a shorter period of 36 months, not 60.
3. LoanChart3 – This graph draws attention to the change in the interest rate indicator by the “Grade” category, as well as taking into account the interest status. It can be seen that the largest number of clients is “Fully paid” (green circle), these clients are subject to the lowest interest rate, which naturally increases as the “grade” category decreases. This trend is characteristic of all interest categories, in this case the interest rate does not depend on the loan amount.
4. LoanChart4 – General line graphs, including the values ​​of “revol_bal”, “total_rec_late_fee”, “total_pyment” and “open_acc”. It can be seen that the values ​​increase with the year, “Fully Paid” is again the highest.
5. RejectedLoanChart1 – This table-type graph shows that DTI (debt-to-income ratio) also directly depends on the length of service of the clients. According to the graph and data, people who work less have the highest DTI index. This table also shows the values ​​​​by US cities, it can be seen that the highest values ​​​​prevail in large cities.
6. RejectedLoanChart2 – From the graph, you can see the trend that the client’s risk level depends on the amount requested to be lent. The larger the loan application, the greater the risk to the bank. There is a visible increase in values ​​starting from 2008 and a significant jump from 2012 to 2013. RejectedLoanChart3 – is a graphical representation of the same insights with additional values.
Good profile criteria – low interest rates compared to other clients, long employment history, bankruptcy cases (success criteria after failure?), A-C risk categories, high/good income history
Semi-good, bad profile criteria – very low number of borrowing interests, data.
Rejected criteria – a unique value is found – “recoveries”, “revol_bal” value changes differently compared to “fully paid”, while the “total_rec_late_fee” graphs coincide with “fully paid”. The degree of risk increases with the amount requested to borrow, less employment history leads to higher risk.

# H2o.ai ML: (for visuals refer to - h2o.ai)
For this task, I chose my pre-organized and concatenated dataset (LoanDataFixedA + LoanDataFixedB), the model choice was Gradient Boosting Machine.
When creating the model, several unnecessary columns were discarded: Addr State, Table Names, Last Pymnt D, and key parameters were selected, such as ntrees – 50 (later changed to 100).
First test of the model with 50 ntrees (the number was increased to 100, because we wanted to see when exactly the logloss value would start to drop)
Training_logloss indicator - a measure of the model's accuracy, fluctuates depending on the number of trees. As the number of ntrees starts to increase, it decreases in the range of 0 - 10 ntrees (on average from 0.0355 to 0.0211), which indicates improving results. The logloss value starts to rise again from an average of 15 ntrees and rises steadily to the highest limit - at 71 ntrees (0.0862), then decreases slightly, but remains higher than in the previous stages - at 100 ntrees (0.0812). This pattern may indicate a tendency to transplant above a certain number of trees.
So we see that a small number of ntrees undoubtedly indicates a higher model accuracy, but this does not make sense, so the optimal choice would be between 70 and 80 ntrees values, since from 85 ntress the accuracy starts to decrease again.
The trend shows that the indicator with the greatest impact on the forecasts is Recoveries, with a value of 1.0. Starting with Int_Rate and other lower indicators, a very large gap is visible – 0.22 and below. The lowest values are such as Pub Rec, Home_Ownership, which is not surprising, since these are not numerical values (perhaps it would have made sense to remove them before building the model).
Charged Off: The model accurately predicts loans that are in default, with a recall of 0.99, which means that 99% of cases were correctly classified.
Fully Paid: The model also identifies fully paid loans well, with a precision of 1.0, which indicates that all actual fully paid cases were correctly identified.
On the other hand, for rarer categories such as Late (16–30 days) and Late (31–120 days), the precision is lower, at 0.87 and 0.83, respectively, indicating that these categories are difficult to accurately predict.
To conclude, the precision scores are generally high across all classes, indicating a low number of false positives across all cases.
Charged Off: The model’s performance in identifying charged loans shows high performance (0.98) and precision (0.99).
Fully paid: For fully paid loans, the model has high recall (0.99) and precision (0.99), indicating that the performance in this category is good.
However, again, the same trend is that the model has difficulty predicting the minority classes “Overdue (16-30 days)” and “Overdue (31-120 days)”), showing zero recall (0.0) and low precision. These are challenging cases.
In summary:
The Gradient Boosting Machine model showed quite good predictive ability to distinguish between the loan statuses “Paid” and “Fully Paid”, and therefore achieved the highest precision in these common categories. However, due to class imbalance, its performance weakened when processing less common states, such as “Late,” resulting in a decrease in cancellation rates and making it difficult for the model to make accurate predictions. Despite these limitations, the model’s performance in distinguishing between defaulted loans and successful repayments highlights its effectiveness in mainstream classifications.
